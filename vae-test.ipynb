{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fastprogress'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcloudcasting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NUM_CHANNELS, DATA_INTERVAL_SPACING_MINUTES\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcloud_diffusion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CloudcastingDataset\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcloud_diffusion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_seed\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcloud_diffusion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mddpm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m noisify_ddpm, ddim_sampler\n",
      "File \u001b[0;32m/bask/projects/v/vjgo8416-climate/users/gmmg6904/cloud_diffusion/cloud_diffusion/dataset.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mT\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastprogress\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m progress_bar\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcloudcasting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMAGE_SIZE_TUPLE, NUM_CHANNELS\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcloudcasting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SatelliteDataset\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fastprogress'"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "\n",
    "from cloudcasting.constants import NUM_CHANNELS, DATA_INTERVAL_SPACING_MINUTES\n",
    "\n",
    "from cloud_diffusion.dataset import CloudcastingDataset\n",
    "from cloud_diffusion.utils import set_seed\n",
    "from cloud_diffusion.ddpm import noisify_ddpm, ddim_sampler\n",
    "from cloud_diffusion.models import UNet2D\n",
    "from cloud_diffusion.vae import TemporalVAEAdapter\n",
    "\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from fastprogress import progress_bar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "channel_names = ['IR_016', 'IR_039', 'IR_087', 'IR_097', 'IR_108', 'IR_120', 'IR_134',\n",
    "       'VIS006', 'VIS008', 'WV_062', 'WV_073']\n",
    "\n",
    "PROJECT_NAME = \"nathan-test\"\n",
    "MERGE_CHANNELS = False\n",
    "DEBUG = True\n",
    "LOCAL = False\n",
    "\n",
    "config = SimpleNamespace(\n",
    "    img_size=256,\n",
    "    epochs=50,  # number of epochs\n",
    "    model_name=\"latent-diffusion\",  # model name to save [unet_small, unet_big]\n",
    "    strategy=\"ddpm\",  # strategy to use [ddpm, simple_diffusion]\n",
    "    noise_steps=1000,  # number of noise steps on the diffusion process\n",
    "    sampler_steps=300,  # number of sampler steps on the diffusion process\n",
    "    seed=42,  # random seed\n",
    "    batch_size=2,  # batch size\n",
    "    device=\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    num_workers=0 if DEBUG else 35,  # number of workers for dataloader\n",
    "    num_frames=4,  # number of frames to use as input (includes noise frame)\n",
    "    lr=5e-4,  # learning rate\n",
    "    log_every_epoch=1,  # log every n epochs to wandb\n",
    "    n_preds=8,  # number of predictions to make\n",
    "    latent_dim=4,\n",
    "    vae_lr=5e-5,\n",
    ")\n",
    "\n",
    "device = config.device\n",
    "\n",
    "\n",
    "HISTORY_STEPS = config.num_frames - 1\n",
    "\n",
    "\n",
    "config.model_params = dict(\n",
    "    block_out_channels=(32, 64, 128, 256),  # number of channels for each block\n",
    "    norm_num_groups=8,  # number of groups for the normalization layer\n",
    "    in_channels=config.num_frames * config.latent_dim,  # number of input channels\n",
    "    out_channels=config.latent_dim,  # number of output channels\n",
    ")\n",
    "\n",
    "set_seed(config.seed)\n",
    "\n",
    "if LOCAL:\n",
    "    TRAINING_DATA_PATH = VALIDATION_DATA_PATH = \"/users/nsimpson/Code/climetrend/cloudcast/2020_training_nonhrv.zarr\"\n",
    "else:\n",
    "    TRAINING_DATA_PATH = \"/bask/projects/v/vjgo8416-climate/shared/data/eumetsat/training/2021_nonhrv.zarr\"\n",
    "    VALIDATION_DATA_PATH = \"/bask/projects/v/vjgo8416-climate/shared/data/eumetsat/training/2022_training_nonhrv.zarr\"\n",
    "# Instantiate the torch dataset object\n",
    "train_ds = CloudcastingDataset(\n",
    "    config.img_size,\n",
    "    valid=False,\n",
    "    # strategy=\"resize\",\n",
    "    zarr_path=TRAINING_DATA_PATH,\n",
    "    start_time=None,\n",
    "    end_time=None,\n",
    "    history_mins=(HISTORY_STEPS - 1) * DATA_INTERVAL_SPACING_MINUTES,\n",
    "    forecast_mins=15,\n",
    "    sample_freq_mins=15,\n",
    "    nan_to_num=False,\n",
    "    merge_channels=MERGE_CHANNELS,\n",
    ")\n",
    "# worth noting they do some sort of shuffling here; we don't for now\n",
    "valid_ds = CloudcastingDataset(\n",
    "    config.img_size,\n",
    "    valid=True,\n",
    "    # strategy=\"resize\",\n",
    "    zarr_path=VALIDATION_DATA_PATH,\n",
    "    start_time=None,\n",
    "    end_time=None,\n",
    "    history_mins=(HISTORY_STEPS - 1) * DATA_INTERVAL_SPACING_MINUTES,\n",
    "    forecast_mins=15,\n",
    "    sample_freq_mins=15,\n",
    "    nan_to_num=False,\n",
    "    merge_channels=MERGE_CHANNELS,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, config.batch_size, shuffle=True,  num_workers=config.num_workers, pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_ds, config.batch_size, shuffle=True, num_workers=config.num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = TemporalVAEAdapter(AutoencoderKL.from_pretrained(\"stabilityai/sdxl-vae\")\n",
    "vae.load_state_dict('f/bask/projects/v/vjgo8416-climate/users/gmmg6904/cloud_diffusion/models/omf5mrig_cloud-finetune--vae.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 11, 4, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# model setup\n",
    "unet = UNet2D(**config.model_params).to(device)\n",
    "# vae = get_hacked_vae().to(device)#.float()\n",
    "from diffusers.models import AutoencoderKL\n",
    "vae = TemporalVAEAdapter(AutoencoderKL.from_pretrained(\"stabilityai/sdxl-vae\").to(device)).to(device)\n",
    "\n",
    "# vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\").to(device).float()\n",
    "# for param in vae.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# sampler\n",
    "sampler = ddim_sampler(steps=config.sampler_steps)\n",
    "\n",
    "# configure training\n",
    "# wandb.config.update(config)\n",
    "config.total_train_steps = config.epochs * len(train_dataloader)\n",
    "\n",
    "# Create parameter groups with different learning rates\n",
    "param_groups = [\n",
    "    {\n",
    "        'params': [p for p in vae.parameters() if p.requires_grad],\n",
    "        'lr': config.vae_lr,\n",
    "        'eps': 1e-5\n",
    "    },\n",
    "    {\n",
    "        'params': unet.parameters(),\n",
    "        'lr': config.lr,\n",
    "        'eps': 1e-5\n",
    "    }\n",
    "]\n",
    "optimizer = AdamW(param_groups)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=[config.vae_lr, config.lr], total_steps=config.total_train_steps)\n",
    "scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "# get a validation batch for logging\n",
    "val_batch = next(iter(valid_dataloader))[0:2].to(device)  # log first 2 predictions\n",
    "print(val_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tensor(tensor, name, print_stats=False):\n",
    "    \"\"\"Utility function to check tensor for NaN/Inf values and optionally print statistics\"\"\"\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"NaN detected in {name}\")\n",
    "        return False\n",
    "    if torch.isinf(tensor).any():\n",
    "        print(f\"Inf detected in {name}\")\n",
    "        return False\n",
    "    if print_stats:\n",
    "        print(f\"{name} stats: min={tensor.min().item():.3f}, max={tensor.max().item():.3f}, \"\n",
    "              f\"mean={tensor.mean().item():.3f}, std={tensor.std().item():.3f}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.isnan(val_batch).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_channels_over_time(images,  batch_idx=0, figsize=(12, 8), cmap='viridis'):\n",
    "    \"\"\"\n",
    "    Visualize multi-channel images over time, handling both single and multiple timesteps.\n",
    "    \n",
    "    Args:\n",
    "        images: Tensor of shape (batch, channels, time, height, width)\n",
    "        channel_names: List of names for each channel\n",
    "        batch_idx: Which batch element to visualize\n",
    "        figsize: Size of the figure\n",
    "        cmap: Colormap to use for visualization\n",
    "    \"\"\"\n",
    "    n_channels = images.shape[1]\n",
    "    n_timesteps = images.shape[2]\n",
    "    \n",
    "\n",
    "    \n",
    "    # Create a grid of subplots\n",
    "    if n_timesteps == 1:\n",
    "        fig, axes = plt.subplots(n_channels, 1, figsize=figsize)\n",
    "        axes = axes.reshape(-1, 1)  # Reshape to 2D array for consistent indexing\n",
    "    else:\n",
    "        fig, axes = plt.subplots(n_channels, n_timesteps, figsize=figsize)\n",
    "    \n",
    "    # Set the spacing between subplots\n",
    "    plt.subplots_adjust(hspace=0.2, wspace=-0.5)\n",
    "    \n",
    "    # Iterate through channels and timesteps\n",
    "    for channel in range(n_channels):\n",
    "        for timestep in range(n_timesteps):\n",
    "            # Get the current image\n",
    "            img = images[batch_idx, channel, timestep].numpy()\n",
    "            \n",
    "            # Normalize the image for better visualization\n",
    "            # img_min = img.min()\n",
    "            # img_max = img.max()\n",
    "            # if img_max > img_min:\n",
    "            #     img = (img - img_min) / (img_max - img_min)\n",
    "            \n",
    "            # Plot the image\n",
    "            im = axes[channel, timestep].imshow(img, cmap=cmap, origin='lower')\n",
    "            axes[channel, timestep].axis('off')\n",
    "            \n",
    "            # Add colorbar\n",
    "            plt.colorbar(im, ax=axes[channel, timestep], fraction=0.046, pad=0.04)\n",
    "            \n",
    "            # Add titles\n",
    "            if channel == 0:\n",
    "                extra = ' (predicted)' if (timestep-3) == 0 else ''\n",
    "                axes[channel, timestep].set_title(f'frame {timestep-3}' + extra)\n",
    "            if timestep == 0:\n",
    "                axes[channel, timestep].text(-10, 32, channel_names[channel], \n",
    "                                          rotation=0, ha='right', va='center')\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from cloud_diffusion.wandb import save_model\n",
    "\n",
    "vae_loss_scale = 1\n",
    "print_stats = False\n",
    "\n",
    "# Modified training loop with checks\n",
    "for epoch in progress_bar(range(config.epochs), total=config.epochs, leave=True):\n",
    "    unet.train()\n",
    "    pbar = progress_bar(train_dataloader, leave=False)\n",
    "    for batch in pbar:\n",
    "        if torch.isnan(batch).all():\n",
    "            continue\n",
    "            \n",
    "        batch = torch.nan_to_num(batch, nan=0)\n",
    "        img_batch = batch.to(device)\n",
    "        \n",
    "    # with torch.autocast(device):   # we want this but NaNs happen in the encoder :(\n",
    "        latents = vae.encode_frames(img_batch)\n",
    "            \n",
    "        past_frames = latents[:, :, :-1]\n",
    "        last_frame = latents[:, :, -1]\n",
    "        \n",
    "        noised_img, t, noise = noisify_ddpm(last_frame)\n",
    "        \n",
    "        past_frames = past_frames.permute(0, 2, 1, 3, 4)\n",
    "        past_frames = past_frames.reshape(latents.shape[0], -1, latents.shape[3], latents.shape[4])\n",
    "        \n",
    "        diffusion_input = torch.cat([past_frames, noised_img], dim=1)\n",
    "        predicted_noise = unet(diffusion_input, t)\n",
    "        diffusion_loss = F.mse_loss(predicted_noise, noise)\n",
    "        \n",
    "        img_batch_hat = vae.decode_frames(latents)\n",
    "        vae_loss = F.mse_loss(img_batch_hat, img_batch)\n",
    "        \n",
    "        loss = diffusion_loss + vae_loss * vae_loss_scale\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        \n",
    "        pbar.comment = f\"epoch={epoch}, vae_loss={vae_loss.item():2.3f}, diffusion_loss={diffusion_loss.item():2.3f}\"\n",
    "\n",
    "    if epoch % config.log_every_epoch == 0:\n",
    "        with torch.no_grad():\n",
    "            val_latents = vae.encode_frames(val_batch)\n",
    "            check_tensor(val_latents, 'val_latents', print_stats=True)\n",
    "            past_val_frames = val_latents[:, :, :-1]\n",
    "            last_val_frame = val_latents[:, :, -1]  # intentionally remove time dim\n",
    "            past_val_frames = past_val_frames.permute(0, 2, 1, 3, 4).reshape(val_latents.shape[0], -1, val_latents.shape[3], val_latents.shape[4])\n",
    "            samples = sampler(unet, past_frames=past_val_frames, num_channels=4)\n",
    "            samples = samples.unsqueeze(dim=2)\n",
    "            check_tensor(samples, 'samples', print_stats=True)\n",
    "            decoded = vae.decode_frames(samples).cpu()\n",
    "\n",
    "        valid_plot = visualize_channels_over_time(torch.cat((val_batch[:,:,:-1].detach().cpu(), decoded), dim=2));\n",
    "        wandb.log({\"all-channels\": valid_plot})\n",
    "\n",
    "save_model(vae, config.model_name + '-unet')\n",
    "save_model(unet, config.model_name + '-vae')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "sys_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
