{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nsimpson/Code/cloud_diffusion/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "\n",
    "from cloudcasting.constants import NUM_CHANNELS, DATA_INTERVAL_SPACING_MINUTES\n",
    "\n",
    "from cloud_diffusion.dataset import CloudcastingDataset\n",
    "from cloud_diffusion.utils import set_seed\n",
    "from cloud_diffusion.ddpm import noisify_ddpm, ddim_sampler\n",
    "from cloud_diffusion.models import UNet2D\n",
    "from cloud_diffusion.vae import get_hacked_vae, encode_frames, decode_frames\n",
    "\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from fastprogress import progress_bar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "channel_names = ['IR_016', 'IR_039', 'IR_087', 'IR_097', 'IR_108', 'IR_120', 'IR_134',\n",
    "       'VIS006', 'VIS008', 'WV_062', 'WV_073']\n",
    "\n",
    "PROJECT_NAME = \"nathan-test\"\n",
    "MERGE_CHANNELS = False\n",
    "DEBUG = True\n",
    "LOCAL = True\n",
    "\n",
    "config = SimpleNamespace(\n",
    "    img_size=256,\n",
    "    epochs=50,  # number of epochs\n",
    "    model_name=\"uvit-test\",  # model name to save [unet_small, unet_big]\n",
    "    strategy=\"ddpm\",  # strategy to use [ddpm, simple_diffusion]\n",
    "    noise_steps=1000,  # number of noise steps on the diffusion process\n",
    "    sampler_steps=300,  # number of sampler steps on the diffusion process\n",
    "    seed=42,  # random seed\n",
    "    batch_size=2,  # batch size\n",
    "    device=\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    num_workers=0 if DEBUG else 8,  # number of workers for dataloader\n",
    "    num_frames=4,  # number of frames to use as input (includes noise frame)\n",
    "    lr=5e-4,  # learning rate\n",
    "    log_every_epoch=1,  # log every n epochs to wandb\n",
    "    n_preds=8,  # number of predictions to make\n",
    "    latent_dim=4,\n",
    "    vae_lr=5e-5,\n",
    ")\n",
    "\n",
    "device = config.device\n",
    "\n",
    "\n",
    "HISTORY_STEPS = config.num_frames - 1\n",
    "\n",
    "\n",
    "config.model_params = dict(\n",
    "    block_out_channels=(32, 64, 128, 256),  # number of channels for each block\n",
    "    norm_num_groups=8,  # number of groups for the normalization layer\n",
    "    in_channels=config.num_frames * config.latent_dim,  # number of input channels\n",
    "    out_channels=config.latent_dim,  # number of output channels\n",
    ")\n",
    "\n",
    "set_seed(config.seed)\n",
    "\n",
    "if LOCAL:\n",
    "    TRAINING_DATA_PATH = VALIDATION_DATA_PATH = \"/users/nsimpson/Code/climetrend/cloudcast/2020_training_nonhrv.zarr\"\n",
    "else:\n",
    "    TRAINING_DATA_PATH = \"/bask/projects/v/vjgo8416-climate/shared/data/eumetsat/training/2021_nonhrv.zarr\"\n",
    "    VALIDATION_DATA_PATH = \"/bask/projects/v/vjgo8416-climate/shared/data/eumetsat/training/2022_training_nonhrv.zarr\"\n",
    "# Instantiate the torch dataset object\n",
    "train_ds = CloudcastingDataset(\n",
    "    config.img_size,\n",
    "    valid=False,\n",
    "    # strategy=\"resize\",\n",
    "    zarr_path=TRAINING_DATA_PATH,\n",
    "    start_time=None,\n",
    "    end_time=None,\n",
    "    history_mins=(HISTORY_STEPS - 1) * DATA_INTERVAL_SPACING_MINUTES,\n",
    "    forecast_mins=15,\n",
    "    sample_freq_mins=15,\n",
    "    nan_to_num=False,\n",
    "    merge_channels=MERGE_CHANNELS,\n",
    ")\n",
    "# worth noting they do some sort of shuffling here; we don't for now\n",
    "valid_ds = CloudcastingDataset(\n",
    "    config.img_size,\n",
    "    valid=True,\n",
    "    # strategy=\"resize\",\n",
    "    zarr_path=VALIDATION_DATA_PATH,\n",
    "    start_time=None,\n",
    "    end_time=None,\n",
    "    history_mins=(HISTORY_STEPS - 1) * DATA_INTERVAL_SPACING_MINUTES,\n",
    "    forecast_mins=15,\n",
    "    sample_freq_mins=15,\n",
    "    nan_to_num=False,\n",
    "    merge_channels=MERGE_CHANNELS,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, config.batch_size, shuffle=True,  num_workers=config.num_workers)\n",
    "valid_dataloader = DataLoader(valid_ds, config.batch_size, shuffle=False, num_workers=config.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 11, 4, 256, 256])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = next(iter(train_dataloader))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfreezing conv_in.weight\n",
      "Unfreezing conv_in.bias\n",
      "Unfreezing conv_out.weight\n",
      "Unfreezing conv_out.bias\n",
      "torch.Size([2, 11, 4, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nsimpson/Code/cloud_diffusion/.venv/lib/python3.12/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# model setup\n",
    "unet = UNet2D(**config.model_params).to(device)\n",
    "vae = get_hacked_vae().to(device)\n",
    "\n",
    "# sampler\n",
    "sampler = ddim_sampler(steps=config.sampler_steps)\n",
    "\n",
    "# configure training\n",
    "# wandb.config.update(config)\n",
    "config.total_train_steps = config.epochs * len(train_dataloader)\n",
    "\n",
    "# Create parameter groups with different learning rates\n",
    "param_groups = [\n",
    "    {\n",
    "        'params': [p for p in vae.parameters() if p.requires_grad],\n",
    "        'lr': config.vae_lr,\n",
    "        'eps': 1e-5\n",
    "    },\n",
    "    {\n",
    "        'params': unet.parameters(),\n",
    "        'lr': config.lr,\n",
    "        'eps': 1e-5\n",
    "    }\n",
    "]\n",
    "optimizer = AdamW(param_groups)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=[config.vae_lr, config.lr], total_steps=config.total_train_steps)\n",
    "scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "# get a validation batch for logging\n",
    "val_batch = next(iter(valid_dataloader))[0:2].to(device)  # log first 2 predictions\n",
    "print(val_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1' class='' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      2.00% [1/50 00:20&lt;16:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1430' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1430 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nsimpson/Code/cloud_diffusion/.venv/lib/python3.12/site-packages/diffusers/models/attention_processor.py:2383: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::_scaled_dot_product_attention_math_for_mps. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/BatchedFallback.cpp:84.)\n",
      "  hidden_states = F.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_frame.shape=torch.Size([2, 4, 32, 32])\n",
      "past_frames.shape=torch.Size([2, 12, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1430' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1430 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_frame.shape=torch.Size([2, 4, 32, 32])\n",
      "past_frames.shape=torch.Size([2, 12, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='48' class='' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      16.00% [48/300 00:02&lt;00:15 DDIM Sampler: frame 756]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 64\u001b[0m\n\u001b[1;32m     62\u001b[0m         last_val_frame \u001b[38;5;241m=\u001b[39m val_latents[:, :, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# intentionally remove time dim\u001b[39;00m\n\u001b[1;32m     63\u001b[0m         past_val_frames \u001b[38;5;241m=\u001b[39m past_val_frames\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(val_latents\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, val_latents\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m], val_latents\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m4\u001b[39m])\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m---> 64\u001b[0m         samples \u001b[38;5;241m=\u001b[39m \u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43munet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_val_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# # log predictions\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# if epoch % config.log_every_epoch == 0:\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m#     samples = ...\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m#     log_images(val_batch, samples)\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# save_model(vae, config.model_name)\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/cloud_diffusion/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/cloud_diffusion/cloud_diffusion/ddpm.py:50\u001b[0m, in \u001b[0;36mdiffusers_sampler\u001b[0;34m(model, past_frames, sched, num_channels, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m     49\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mcomment \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDDIM Sampler: frame \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 50\u001b[0m     noise \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpast_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_frame\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     new_frame \u001b[38;5;241m=\u001b[39m sched\u001b[38;5;241m.\u001b[39mstep(noise, t, new_frame, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mprev_sample\n\u001b[1;32m     52\u001b[0m     preds\u001b[38;5;241m.\u001b[39mappend(new_frame\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mcpu())\n",
      "File \u001b[0;32m~/Code/cloud_diffusion/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/cloud_diffusion/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Code/cloud_diffusion/cloud_diffusion/models.py:77\u001b[0m, in \u001b[0;36mUNet2D.forward\u001b[0;34m(self, *x, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mx, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n",
      "File \u001b[0;32m~/Code/cloud_diffusion/.venv/lib/python3.12/site-packages/diffusers/models/unets/unet_2d.py:313\u001b[0m, in \u001b[0;36mUNet2DModel.forward\u001b[0;34m(self, sample, timestep, class_labels, return_dict)\u001b[0m\n\u001b[1;32m    309\u001b[0m         sample, res_samples, skip_sample \u001b[38;5;241m=\u001b[39m downsample_block(\n\u001b[1;32m    310\u001b[0m             hidden_states\u001b[38;5;241m=\u001b[39msample, temb\u001b[38;5;241m=\u001b[39memb, skip_sample\u001b[38;5;241m=\u001b[39mskip_sample\n\u001b[1;32m    311\u001b[0m         )\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 313\u001b[0m         sample, res_samples \u001b[38;5;241m=\u001b[39m \u001b[43mdownsample_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     down_block_res_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res_samples\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# 4. mid\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/cloud_diffusion/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/cloud_diffusion/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Code/cloud_diffusion/.venv/lib/python3.12/site-packages/diffusers/models/unets/unet_2d_blocks.py:1142\u001b[0m, in \u001b[0;36mAttnDownBlock2D.forward\u001b[0;34m(self, hidden_states, temb, upsample_size, cross_attention_kwargs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m             hidden_states \u001b[38;5;241m=\u001b[39m downsampler(hidden_states, temb\u001b[38;5;241m=\u001b[39mtemb)\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1142\u001b[0m             hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mdownsampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1144\u001b[0m     output_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states, output_states\n",
      "File \u001b[0;32m~/Code/cloud_diffusion/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/cloud_diffusion/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Code/cloud_diffusion/.venv/lib/python3.12/site-packages/diffusers/models/downsampling.py:147\u001b[0m, in \u001b[0;36mDownsample2D.forward\u001b[0;34m(self, hidden_states, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(hidden_states, pad, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels\n\u001b[0;32m--> 147\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/Code/cloud_diffusion/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/cloud_diffusion/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Code/cloud_diffusion/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/cloud_diffusion/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "vae.half()\n",
    "\n",
    "for epoch in progress_bar(range(config.epochs), total=config.epochs, leave=True):\n",
    "    vae.train()\n",
    "    unet.train()\n",
    "    pbar = progress_bar(train_dataloader, leave=False)\n",
    "    for batch in pbar:\n",
    "        if torch.isnan(batch).all():\n",
    "            continue\n",
    "\n",
    "        batch = torch.as_tensor(batch, dtype=torch.float16)\n",
    "\n",
    "        # (batch, channels, time, height, width)\n",
    "        batch = torch.nan_to_num(batch, nan=0)\n",
    "        img_batch = batch.to(device)\n",
    "        with torch.autocast(device):\n",
    "\n",
    "            # vae math\n",
    "            latents = encode_frames(img_batch, vae)\n",
    "           \n",
    "            # diffusion math\n",
    "            past_frames = latents[:, :, :-1]\n",
    "            last_frame = latents[:, :, -1]  # intentionally remove time dim\n",
    "\n",
    "            noised_img, t, noise = noisify_ddpm(last_frame)\n",
    "\n",
    "            # flip the time and channel dimensions before merging them.\n",
    "            # for time = t, channel = c, the order of the flattened input is now:\n",
    "            # [t1c1, t1c2, ..., t2c1, t2c2, ...]\n",
    "            # I don't know if this makes a difference, but it keeps things more consistent!\n",
    "            past_frames = past_frames = past_frames.permute(0, 2, 1, 3, 4).reshape(latents.shape[0], -1, latents.shape[3], latents.shape[4])\n",
    "            # concatenate on channel dim\n",
    "            diffusion_input = torch.cat([past_frames, noised_img], dim=1)\n",
    "\n",
    "            # diffusion loss calc\n",
    "            predicted_noise = unet(diffusion_input, t)\n",
    "            diffusion_loss = F.mse_loss(predicted_noise, noise)\n",
    "\n",
    "            # also calculate a reconstruction loss for the vae on our encoded frames\n",
    "            latents = latents.half()\n",
    "            img_batch_hat = decode_frames(latents, vae)\n",
    "            vae_loss = F.mse_loss(img_batch_hat, img_batch)\n",
    "\n",
    "            # total loss (arbitrary weighting)\n",
    "            loss = diffusion_loss + 0.1*vae_loss\n",
    "\n",
    "            break\n",
    "\n",
    "            # backprop \n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "            # wandb.log({\"train_mse\": loss.item(), \"learning_rate\": scheduler.get_last_lr()[0], \"vae_loss\": vae_loss.item(), \"diffusion_loss\": diffusion_loss.item()})\n",
    "        pbar.comment = f\"epoch={epoch}, vae_loss={vae_loss.item():2.3f}, diffusion_loss={diffusion_loss.item():2.3f}\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_latents = encode_frames(val_batch.half(), vae)\n",
    "        past_val_frames = val_latents[:, :, :-1]\n",
    "        last_val_frame = val_latents[:, :, -1]  # intentionally remove time dim\n",
    "        past_val_frames = past_val_frames.permute(0, 2, 1, 3, 4).reshape(val_latents.shape[0], -1, val_latents.shape[3], val_latents.shape[4]).float()\n",
    "        samples = sampler(unet, past_frames=past_val_frames, num_channels=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # # log predictions\n",
    "    # if epoch % config.log_every_epoch == 0:\n",
    "    #     samples = ...\n",
    "    #     log_images(val_batch, samples)\n",
    "\n",
    "# save_model(vae, config.model_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_channels_over_time(images,  batch_idx=0, figsize=(12, 8), cmap='viridis'):\n",
    "    \"\"\"\n",
    "    Visualize multi-channel images over time, handling both single and multiple timesteps.\n",
    "    \n",
    "    Args:\n",
    "        images: Tensor of shape (batch, channels, time, height, width)\n",
    "        channel_names: List of names for each channel\n",
    "        batch_idx: Which batch element to visualize\n",
    "        figsize: Size of the figure\n",
    "        cmap: Colormap to use for visualization\n",
    "    \"\"\"\n",
    "    n_channels = images.shape[1]\n",
    "    n_timesteps = images.shape[2]\n",
    "    \n",
    "\n",
    "    \n",
    "    # Create a grid of subplots\n",
    "    if n_timesteps == 1:\n",
    "        fig, axes = plt.subplots(n_channels, 1, figsize=figsize)\n",
    "        axes = axes.reshape(-1, 1)  # Reshape to 2D array for consistent indexing\n",
    "    else:\n",
    "        fig, axes = plt.subplots(n_channels, n_timesteps, figsize=figsize)\n",
    "    \n",
    "    # Set the spacing between subplots\n",
    "    plt.subplots_adjust(hspace=0.2, wspace=-0.5)\n",
    "    \n",
    "    # Iterate through channels and timesteps\n",
    "    for channel in range(n_channels):\n",
    "        for timestep in range(n_timesteps):\n",
    "            # Get the current image\n",
    "            img = images[batch_idx, channel, timestep].numpy()\n",
    "            \n",
    "            # Normalize the image for better visualization\n",
    "            # img_min = img.min()\n",
    "            # img_max = img.max()\n",
    "            # if img_max > img_min:\n",
    "            #     img = (img - img_min) / (img_max - img_min)\n",
    "            \n",
    "            # Plot the image\n",
    "            im = axes[channel, timestep].imshow(img, cmap=cmap, origin='lower')\n",
    "            axes[channel, timestep].axis('off')\n",
    "            \n",
    "            # Add colorbar\n",
    "            plt.colorbar(im, ax=axes[channel, timestep], fraction=0.046, pad=0.04)\n",
    "            \n",
    "            # Add titles\n",
    "            if channel == 0:\n",
    "                axes[channel, timestep].set_title(f'frame {timestep-3}')\n",
    "            if timestep == 0:\n",
    "                axes[channel, timestep].text(-10, 32, channel_names[channel], \n",
    "                                          rotation=0, ha='right', va='center')\n",
    "\n",
    "    return fig\n",
    "visualize_channels_over_time(X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_sizes(ground_truth, prediction, method='crop'):\n",
    "    \"\"\"\n",
    "    Match the sizes of ground truth and prediction arrays.\n",
    "    \n",
    "    Args:\n",
    "        ground_truth: Tensor of shape (batch, channels, height1, width1)\n",
    "        prediction: Tensor of shape (batch, channels, height2, width2)\n",
    "        method: One of ['crop', 'pad', 'resize']\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of tensors with matched sizes\n",
    "    \"\"\"\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    if method == 'crop':\n",
    "        # Crop ground truth to prediction size\n",
    "        h_diff = ground_truth.shape[2] - prediction.shape[2]\n",
    "        w_diff = ground_truth.shape[3] - prediction.shape[3]\n",
    "        \n",
    "        h_start = h_diff // 2\n",
    "        w_start = w_diff // 2\n",
    "        \n",
    "        gt_matched = ground_truth[:, :, \n",
    "                                h_start:h_start + prediction.shape[2],\n",
    "                                w_start:w_start + prediction.shape[3]]\n",
    "        pred_matched = prediction\n",
    "        \n",
    "    elif method == 'pad':\n",
    "        # Pad prediction to ground truth size\n",
    "        h_diff = ground_truth.shape[2] - prediction.shape[2]\n",
    "        w_diff = ground_truth.shape[3] - prediction.shape[3]\n",
    "        \n",
    "        h_pad = (h_diff // 2, h_diff - h_diff // 2)\n",
    "        w_pad = (w_diff // 2, w_diff - w_diff // 2)\n",
    "        \n",
    "        pred_matched = F.pad(prediction, (w_pad[0], w_pad[1], h_pad[0], h_pad[1]))\n",
    "        gt_matched = ground_truth\n",
    "        \n",
    "    elif method == 'resize':\n",
    "        # Resize prediction to ground truth size using bilinear interpolation\n",
    "        pred_matched = F.interpolate(prediction, \n",
    "                                   size=(ground_truth.shape[2], ground_truth.shape[3]),\n",
    "                                   mode='bilinear',\n",
    "                                   align_corners=False)\n",
    "        gt_matched = ground_truth\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"method must be one of ['crop', 'pad', 'resize']\")\n",
    "        \n",
    "    return gt_matched, pred_matched\n",
    "\n",
    "def visualize_prediction_comparison(ground_truth, prediction, batch_idx=0, figsize=(20, 8), \n",
    "                                  cmap='viridis', diff_cmap='RdBu_r', size_match='crop'):\n",
    "    \"\"\"\n",
    "    Visualize ground truth, prediction, and their differences side by side for each channel.\n",
    "    Maintains original data ranges for proper visualization.\n",
    "    \"\"\"\n",
    "    # Match sizes\n",
    "    gt_matched, pred_matched = match_sizes(ground_truth, prediction, method=size_match)\n",
    "    n_channels = ground_truth.shape[1]\n",
    "\n",
    "    pred_matched[torch.isnan(gt_matched)] = torch.nan\n",
    "\n",
    "    # Create a grid of subplots\n",
    "    fig, axes = plt.subplots(n_channels, 3, figsize=figsize)\n",
    "\n",
    "    # Set the spacing between subplots\n",
    "    plt.subplots_adjust(hspace=0.2, wspace=-0.8)\n",
    "    \n",
    "    # Iterate through channels\n",
    "    for channel in range(n_channels):\n",
    "        # Get images\n",
    "        gt_img = gt_matched[batch_idx, channel].numpy()\n",
    "        pred_img = pred_matched[batch_idx, channel].numpy()\n",
    "        diff_img = pred_img - gt_img\n",
    "        \n",
    "        # For difference plot, use symmetric limits around zero\n",
    "        diff_bound = max(abs(diff_img.min()), abs(diff_img.max()))\n",
    "        diff_bound = max(diff_bound, 0.1)  # Ensure some minimum range for visualization\n",
    "        \n",
    "        # Plot ground truth with original range\n",
    "        im_gt = axes[channel, 0].imshow(gt_img, cmap=cmap, origin='lower')\n",
    "        axes[channel, 0].axis('off')\n",
    "        plt.colorbar(im_gt, ax=axes[channel, 0], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # Plot prediction with same range as ground truth\n",
    "        im_pred = axes[channel, 1].imshow(pred_img, cmap=cmap, origin='lower')\n",
    "        axes[channel, 1].axis('off')\n",
    "        plt.colorbar(im_pred, ax=axes[channel, 1], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # Plot difference with symmetric range\n",
    "        im_diff = axes[channel, 2].imshow(diff_img, cmap=diff_cmap, origin='lower',\n",
    "                                        vmin=-diff_bound, vmax=diff_bound)\n",
    "        axes[channel, 2].axis('off')\n",
    "        plt.colorbar(im_diff, ax=axes[channel, 2], fraction=0.046, pad=0.04)\n",
    "        \n",
    "        # Add channel name on the left\n",
    "        axes[channel, 0].text(-10, 32, channel_names[channel],\n",
    "                            rotation=0, ha='right', va='center')\n",
    "    \n",
    "    # Add column headers\n",
    "    axes[0, 0].set_title(f'Ground Truth ({gt_matched.shape[2]}x{gt_matched.shape[3]})')\n",
    "    axes[0, 1].set_title(f'Prediction ({pred_matched.shape[2]}x{pred_matched.shape[3]})')\n",
    "    axes[0, 2].set_title('Difference (Pred - GT)')\n",
    "    return fig\n",
    "\n",
    "visualize_prediction_comparison(img_batch[:, :,  :, :], res[:, :, :, :], size_match=\"resize\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
