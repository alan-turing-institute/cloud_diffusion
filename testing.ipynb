{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nsimpson/Code/cloud_diffusion/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mphinate\u001b[0m (\u001b[33mmanchester_prize\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/nsimpson/Code/cloud_diffusion/wandb/run-20241023_142917-1q8fdlxz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/manchester_prize/ddpm_clouds/runs/1q8fdlxz' target=\"_blank\">northern-fog-2</a></strong> to <a href='https://wandb.ai/manchester_prize/ddpm_clouds' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/manchester_prize/ddpm_clouds' target=\"_blank\">https://wandb.ai/manchester_prize/ddpm_clouds</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/manchester_prize/ddpm_clouds/runs/1q8fdlxz' target=\"_blank\">https://wandb.ai/manchester_prize/ddpm_clouds/runs/1q8fdlxz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact np_dataset:v0, 3816.62MB. 30 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   30 of 30 files downloaded.  \n",
      "Done. 0:6:44.4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">northern-fog-2</strong> at: <a href='https://wandb.ai/manchester_prize/ddpm_clouds/runs/1q8fdlxz' target=\"_blank\">https://wandb.ai/manchester_prize/ddpm_clouds/runs/1q8fdlxz</a><br/> View project at: <a href='https://wandb.ai/manchester_prize/ddpm_clouds' target=\"_blank\">https://wandb.ai/manchester_prize/ddpm_clouds</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241023_142917-1q8fdlxz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.6665e-02,  6.3076e-02,  5.8489e-02,  ..., -2.7529e-01,\n",
       "          -2.9290e-01, -3.0397e-01],\n",
       "         [ 1.6778e-04,  5.4600e-02,  5.6731e-02,  ..., -2.9674e-01,\n",
       "          -3.0536e-01, -3.1290e-01],\n",
       "         [ 1.9444e-02,  4.4553e-02,  3.7216e-02,  ..., -3.0977e-01,\n",
       "          -3.2065e-01, -3.2652e-01],\n",
       "         ...,\n",
       "         [-3.3933e-01, -3.2881e-01, -2.5199e-01,  ..., -3.1279e-01,\n",
       "          -3.1407e-01, -3.1473e-01],\n",
       "         [-3.3650e-01, -3.2294e-01, -2.7958e-01,  ..., -3.1458e-01,\n",
       "          -3.1538e-01, -3.1643e-01],\n",
       "         [-3.2120e-01, -2.8458e-01, -2.8682e-01,  ..., -3.1503e-01,\n",
       "          -3.1591e-01, -3.1746e-01]],\n",
       "\n",
       "        [[ 3.1216e-02,  2.7759e-02,  3.9924e-02,  ..., -2.5125e-01,\n",
       "          -2.8785e-01, -2.9981e-01],\n",
       "         [ 3.0683e-02,  3.7654e-02,  3.7798e-02,  ..., -2.2979e-01,\n",
       "          -2.9156e-01, -3.0760e-01],\n",
       "         [ 2.6864e-02,  3.3317e-02,  2.5960e-02,  ..., -1.8531e-01,\n",
       "          -2.6132e-01, -3.0356e-01],\n",
       "         ...,\n",
       "         [-3.2504e-01, -3.3089e-01, -3.1451e-01,  ..., -3.1203e-01,\n",
       "          -3.1376e-01, -3.1510e-01],\n",
       "         [-3.1190e-01, -3.2602e-01, -3.2040e-01,  ..., -3.1328e-01,\n",
       "          -3.1487e-01, -3.1717e-01],\n",
       "         [-2.9389e-01, -3.0014e-01, -3.0698e-01,  ..., -3.1499e-01,\n",
       "          -3.1458e-01, -3.1605e-01]],\n",
       "\n",
       "        [[-4.1169e-02,  3.3096e-02,  5.9103e-02,  ..., -3.9999e-02,\n",
       "          -1.0289e-01, -1.8562e-01],\n",
       "         [-6.9400e-02,  1.1367e-02,  4.9608e-02,  ..., -4.7104e-02,\n",
       "          -7.8658e-02, -1.3073e-01],\n",
       "         [-5.3567e-02,  2.9979e-03,  1.9573e-02,  ..., -3.4547e-02,\n",
       "          -4.0207e-02, -6.4064e-02],\n",
       "         ...,\n",
       "         [-1.7961e-01, -2.2899e-01, -2.5185e-01,  ..., -3.1128e-01,\n",
       "          -3.1282e-01, -3.1447e-01],\n",
       "         [-1.5990e-01, -2.4161e-01, -2.7405e-01,  ..., -3.1273e-01,\n",
       "          -3.1477e-01, -3.1584e-01],\n",
       "         [-1.7588e-01, -2.6326e-01, -2.9149e-01,  ..., -3.1368e-01,\n",
       "          -3.1540e-01, -3.1618e-01]],\n",
       "\n",
       "        [[ 2.0071e-02,  2.5042e-03,  5.0675e-04,  ...,  1.5585e-02,\n",
       "          -2.6186e-02, -8.5444e-02],\n",
       "         [ 1.9144e-02,  1.3848e-02,  2.6032e-02,  ...,  1.6289e-02,\n",
       "          -8.7073e-03, -3.9456e-02],\n",
       "         [ 2.3545e-02,  5.7235e-02,  8.2393e-02,  ...,  1.1077e-02,\n",
       "           5.0835e-03, -8.5512e-03],\n",
       "         ...,\n",
       "         [-5.7925e-02, -4.5480e-02, -1.2005e-01,  ..., -3.1171e-01,\n",
       "          -3.1236e-01, -3.1316e-01],\n",
       "         [-6.8554e-02, -6.1745e-02, -1.2288e-01,  ..., -3.1302e-01,\n",
       "          -3.1369e-01, -3.1536e-01],\n",
       "         [-9.2063e-02, -1.1315e-01, -1.7997e-01,  ..., -3.1407e-01,\n",
       "          -3.1402e-01, -3.1611e-01]]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "import torch\n",
    "\n",
    "from cloud_diffusion.dataset import download_dataset, CloudDataset\n",
    "from cloud_diffusion.ddpm import noisify_ddpm\n",
    "\n",
    "\n",
    "PROJECT_NAME = \"ddpm_clouds\"\n",
    "DATASET_ARTIFACT = \"capecape/gtc/np_dataset:v0\"\n",
    "\n",
    "config = SimpleNamespace(\n",
    "    epochs=50,  # number of epochs\n",
    "    model_name=\"unet_small\",  # model name to save [unet_small, unet_big]\n",
    "    strategy=\"ddpm\",  # strategy to use ddpm\n",
    "    noise_steps=1000,  # number of noise steps on the diffusion process\n",
    "    sampler_steps=333,  # number of sampler steps on the diffusion process\n",
    "    seed=42,  # random seed\n",
    "    batch_size=128,  # batch size\n",
    "    img_size=64,  # image size\n",
    "    device=\"cuda\",  # device\n",
    "    num_workers=8,  # number of workers for dataloader\n",
    "    num_frames=4,  # number of frames to use as input\n",
    "    lr=5e-4,  # learning rate\n",
    "    validation_days=3,  # number of days to use for validation\n",
    "    log_every_epoch=5,  # log every n epochs to wandb\n",
    "    n_preds=8,  # number of predictions to make\n",
    ")\n",
    "\n",
    "# downlaod the dataset from the wandb.Artifact\n",
    "files = download_dataset(DATASET_ARTIFACT, PROJECT_NAME)\n",
    "train_days, valid_days = files[: -config.validation_days], files[-config.validation_days :]\n",
    "train_ds = CloudDataset(files=train_days, num_frames=config.num_frames, img_size=config.img_size)\n",
    "valid_ds = CloudDataset(files=valid_days, num_frames=config.num_frames, img_size=config.img_size).shuffle()\n",
    "\n",
    "next(iter(valid_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ds = CloudDataset(files=train_days, num_frames=config.num_frames, img_size=446)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 446, 446])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_ds)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "from cloudcasting.constants import IMAGE_SIZE_TUPLE\n",
    "\n",
    "\n",
    "class CloudcastingDataset(SatelliteDataset):\n",
    "    def __init__(self, img_size, valid=False, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        tfms = [T.Resize((img_size, int(img_size * (IMAGE_SIZE_TUPLE[1] / IMAGE_SIZE_TUPLE[0]))))] if img_size is not None else []\n",
    "        tfms += [T.RandomCrop(img_size)] if not valid else [T.CenterCrop(img_size)]\n",
    "        self.tfms = T.Compose(tfms)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        # concatenate future prediction and previous frames along time axis\n",
    "        concat_data = np.concatenate(super().__getitem__(idx), axis=-3)\n",
    "        # data is in [0,1] range, normalize to [-0.5, 0.5]\n",
    "        # note that -1s could be NaNs, which are now at +1.5\n",
    "        # output has shape (11, history_steps + forecast_horizon, height, width)\n",
    "        return 0.5 - self.tfms(torch.from_numpy(concat_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CloudDataset(SatelliteDataset):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.cat(self.data[idx], dim=-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "x = np.ones((2, 3, 4, 5, 6))\n",
    "\n",
    "x[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5, 6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, 0, ...].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 3, 4, 5, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, None, ...].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_data = torch.randn(10, 11, 4, 64, 64, requires_grad=False)\n",
    "\n",
    "betamin, betamax, n_steps = 0.0001, 0.02, 1000\n",
    "beta = torch.linspace(betamin, betamax, n_steps)\n",
    "alpha = 1.0 - beta\n",
    "alphabar = alpha.cumprod(dim=0)\n",
    "sigma = beta.sqrt()\n",
    "\n",
    "\n",
    "def noisify_ddpm(x0):\n",
    "    \"Noise by ddpm\"\n",
    "    device = x0.device\n",
    "    n = len(x0)\n",
    "    t = torch.randint(0, n_steps, (n,), dtype=torch.long)\n",
    "    ε = torch.randn(x0.shape, device=device)\n",
    "    ᾱ_t = alphabar[t].reshape(-1, 1, 1, 1).to(device)\n",
    "    xt = ᾱ_t.sqrt() * x0 + (1 - ᾱ_t).sqrt() * ε\n",
    "    return xt, t.to(device), ε\n",
    "\n",
    "\n",
    "from torch import vmap\n",
    "\n",
    "\n",
    "def noisify_last_frame_channels(frames, noise_func):\n",
    "    \"Noisify the last frame of a sequence. Inputs have shape (batch, channels, time, height, width).\"\n",
    "    past_frames = frames[:, :, :-1]\n",
    "    last_frame = frames[:, :, -1:]\n",
    "\n",
    "    # vmap over channels (dim=1) -- idk why output dim = 1 doesn't work, but this does!\n",
    "    # our out dims are (channels, batch, time, height, width), so we rejig later\n",
    "    # the None will just not vmap over the returned diffusion step counts (called t)\n",
    "    channel_noisify = vmap(noise_func, in_dims=1, out_dims=(0, None, 0), randomness=\"same\")\n",
    "    noise, t, e = channel_noisify(last_frame)\n",
    "\n",
    "    # reshape to (batch, channels, time, height, width) ready for diffusion model, both for noise and e\n",
    "    # leave channels intact for now\n",
    "    noise = torch.swapaxes(noise, 0, 1)\n",
    "    history_and_noisy_target = torch.cat([past_frames, noise], dim=2)\n",
    "    history_and_noisy_target = history_and_noisy_target.view(\n",
    "        history_and_noisy_target.shape[0],\n",
    "        history_and_noisy_target.shape[1] * history_and_noisy_target.shape[2],  # collapse channels and time\n",
    "        history_and_noisy_target.shape[3],\n",
    "        history_and_noisy_target.shape[4],\n",
    "    )\n",
    "\n",
    "    e = torch.swapaxes(e, 0, 1)\n",
    "    e = e.view(e.shape[0], e.shape[1] * e.shape[2], e.shape[3], e.shape[4])\n",
    "\n",
    "    return history_and_noisy_target, t, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5776,  0.6119, -0.5367,  0.0000, -0.1512,  0.9278,  0.7324, -0.0000,\n",
       "        -0.2882,  2.1622,  2.3457,  0.0000, -0.9894, -1.6108,  0.8110,  0.0000,\n",
       "        -1.8311, -0.9710, -0.7868,  0.0000,  0.0486, -1.0543,  1.3018, -0.0000,\n",
       "         0.9455, -0.1048, -1.1720,  0.0000, -0.3532, -0.3649, -1.0455, -0.0000,\n",
       "         0.2589, -0.5421,  0.4593,  0.0000, -0.9825,  0.3658, -1.3455,  0.0000,\n",
       "         1.2885, -0.3904, -0.8328,  0.0000])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisify_last_frame_channels(dummy_data, lambda x: (0 * x, 1, 0 * x))[0][0, :, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 1, 64, 64])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_data[:, -1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
